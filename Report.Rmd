---
title: "Space Shuttle Safety and the *Challenger*"
subtitle: "Statistical Modeling for Critical Decisions"
author : "Aidan Jackson, Sandip Panesar, Devesh Khandelwal"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
---

\newpage

# Investigation of the 1986 Space Shuttle *Challenger* Accident 

## Summary

This exercise examines the Space Shuttle *Challenger*'s 1986 disaster using the previous safety record of the Space Shuttle Program. The probability of O-ring failures at various launch temperatures are modeled to make a prediction for the conditions on the morning of the disaster. It is found that the probability of a catastrophic failure is XX%, over XX times higher than when using the methods of the original estimate. The Rogers Commission, tasked with investigating the accident, noted that this underestimate occurred due to omitting launch data where no O-ring damage was found.$^{[2]}$ The original estimate procedure was used by [Morton Thiokol](https://en.wikipedia.org/wiki/Thiokol), a contractor to NASA and producer of the O-ring involved component that failed. 

This work was originally completed as part of the W271 Statistical Methods for Discrete Response, Time Series, and Panel Data course in the Master of Information and Data Science program at University of California, Berkeley. The exercise in turn was based on a previous 1989 paper on the same topic, originally published in the Journal of the American Statistical Association and included in this repo as a pdf.$^{[1]}$

## Background

On January 28th, 1986, the *Challenger* launched from Cape Canaveral, Florida, for what was the 25th orbital mission of the Space Shuttle program. 59 seconds after launch, a tracking camera recorded a burning plume coming out of the side of the right solid rocket booster (SRB). As the leaking fuel continued to burn, gradually other parts of the shuttle stack began to disintegrate. 73 seconds after launch, still visible in the sky, the shuttle exploded as different fuel sources ignited. This was particularly shocking both for being the first American astronaut deaths in a launch mission^[Apollo 1 resulted in 3 astronaut deaths during pre-launch training on the ground. Soyuz 1 and 11 resulted in a combined 4 cosmonaut deaths due to hardware failures during each launch mission.], as well as its public broadcasting as part of the inaugural [Teacher in Space Project](https://en.wikipedia.org/wiki/Teacher_in_Space_Project). 

In the ensuing investigation, the cause of the accident was found to be O-ring failure in the field-joint of the same right SRB.$^{[2]}$ O-rings are doughnut-shaped and commonly made of rubber, and provide a flexible seal between mechanical components. In a typical car engine, O-rings have diameters on the scale of inches. Inside the Space Shuttles' SRBs, O-ring diameter was 37.5 feet.$^{[1]}$ 

Like many other applications, the SRBs' O-rings sealed against gas to prevent it from escaping where mechanical joints connected. Every Shuttle launch required 2 SRBs, with each SRB containing 3 field-joints that used O-ring seals. For redundancy, each field-joint had both a primary and a secondary O-ring. This resulted in 6 field-joint O-rings total per SRB, or 12 field-joint O-rings total per launch. 

Redundancy is needed because of the potential for an O-ring to fail. There are two mechanisms which could cause this, known as erosion and blow by. Erosion has the potential to occur during normal operation, where hot gases sealed against the O-ring gradually disintegrate it. On the SRBs, putty was added to provide a protective barrier against this mechanism. Blow by occurs when an O-ring already fails to provide a complete seal, allowing high velocity gas to move past the O-ring and damage it further. 

Although the secondary O-rings were a backup should distress and failure occur in a primary O-ring, before the first ever Space Shuttle launch it was discovered that additional problems could also occur. This was due to field-joint design which sometimes caused unintended rotation among different components, including the O-rings. While the primary O-rings would engage a seal reliably even with this rotation, the secondary O-rings specifically could sometimes slip and leave permanent space for gas leakage. If this occurred during a launch where the primary O-ring failed due to the previous distresses, then it was known the result would be a "catastrophic failure".$^{[1]}$

Being made of rubber-like material, there are a variety of conditions which may impact the ability of any O-ring to provide a complete seal and suffer distress. One which was investigated for the Space Shuttle program was temperature, where colder conditions can cause an O-ring to shrink and lose its elasticity. During the late 1970s and early 1980s, the SRB engine with the O-rings was qualified for temperatures as low as 40°F. 

Without any additional statistics, when the Challenger started its Winter morning launch it was 36°F.

```{r, message = FALSE}
library(data.table)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(mcprofile)
```

## Exploratory Data Analysis

As with any dataset, understanding what information is available is a prerequisite to deciding on what value to extract from it. Although the size of this dataset is small, with only 23 observations from previous Shuttle launches, the choice of how to treat it is equally as important and influential to later analysis.

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r, message = FALSE}
d <- read.csv("challenger.csv")
d <- data.table(d)

grid.table(head(d), rows = NULL)
```

With the data loaded, it can be seen that there are `r length(d)` variables and `r nrow(d)` observations. Between these, there are `r sum(is.na(d))` missing values. In total there is a small amount of information available, which is to be expected with rare events such as Space Shuttle launches. However, it provides an opportunity to more thoroughly examine all aspects of the dataset.

To begin, the meaning of each variable must be properly understood. `Flight` represents a generic index for each launch. `O.ring` represents the number of primary field-joint O-rings which experienced a "thermal distress" event for each flight. A thermal distress event was defined as when either the effects of erosion or blow by could be observed in the recovered SRBs post launch. Although every launch had 12 field-joint O-rings in total, only six of these are primary while the second six are secondary. This was to design for redundancy, even though mechanisms were found which could lead to the immediate failure of the secondary O-rings should the primary fail as mentioned in **Background**. As confirmation of the total, the variable `Number` represents the count of primary field-joint O-rings and is constant at six for all launches. `Temp` and `Pressure` represent the recorded field-joint temperature in Fahrenheit and pressure in psi respectively. `Temp` is recorded to the nearest degree, while `Pressure` is recorded to the nearest 50 psi (e.g. 50, 100, 150, etc). With a much coarser grain of pressure measurements, it may be less meaningful since less specific information is recorded.

Being the cause of the accident, the `O.ring` variable will be examined to start. In this analysis, unless stated otherwise "O-ring" will refer specifically to a primary field-joint O-ring.

```{r}
ggplot(d, aes(O.ring)) +
  geom_histogram(binwidth = 1, fill = "black", color='black', alpha=0.8) +
  ggtitle("Figure 1. O-Ring Distribution") +
  xlab("Count of O-Ring Thermal Distress Events per Launch") +
  ylab("Count")
```

Shown above in **Figure 1**, the majority of launches had O-rings which experienced no thermal distress events. About 5 launches had a single O-ring experience thermal distress, while even fewer had two. None of the recorded launches had greater than two O-rings distress events. However, for a catastrophic failure to occur, all that was potentially needed would be a single primary O-ring failure to occur when its secondary backup had rotated and lost contact for a seal. 

With the majority of launches not experiencing O-ring distress, those that experienced any distress at all can be grouped together for a more discrete analysis.

```{r}
d$distress <- d$O.ring > 0

grid.table(tail(d), rows = NULL)
```

To better differentiate between these two situations, a new Boolean variable named `distress` is created which indicates whether a launch had at least one O-ring distress event or not. With two potential predictors of these failures, `Temp` and `Pressure`, it will be important to understand how they relate to the distribution of the outcome.

```{r, message = FALSE}
ggplot(d, aes(factor(distress), Temp)) +
  geom_boxplot(aes(fill = factor(distress))) +
  scale_fill_brewer(palette = "Set1", direction = -1) +
  theme(legend.position = "none") +
  geom_jitter() +
  ggtitle("Figure 2. Temperature vs. O-Ring Distress Event") +
  xlab("O-Ring Distress Event") + 
  ylab("Temperature (°F)")
```

Demonstrated in **Figure 2**, O-Ring distress events generally occurred at lower launch temperatures. In fact, with so few total observations, it can be seen that over half of the launches with O-ring distress were at lower temperatures than the coldest launch with no O-ring distress There is a single outlier that is notable, however, where a distress event occurred at a launch temperature of ~75°F. This temperature would be greater than average even among launches which experienced no distress. Note that because of this point, along with generally having a wider distribution, there is greater temperature variance among launches which had distress events compared to those which did not. 

The other environmental variable in the data set, `Pressure`, may also be examined to see if insightful relationships emerge. 

```{r}
ggplot(data = d, aes(Temp, O.ring, color = Pressure)) +
  geom_point() +
  ggtitle("Figure 3. O-Ring Failures vs. Environmental Conditions") +
  xlab("Temperature (°F)") +
  ylab("O-Ring Distress Events") +
  labs(size = "Pressure (psi)") +
  scale_color_gradient(low = "blue", high = "red") +
  scale_y_continuous(breaks = c(0, 1, 2)) +
  scale_x_continuous(breaks = c(50, 60, 70, 80))
```

**Figure 3** shows that generally the launches with a greater number of O-ring distress events occurred at lower temperatures and higher pressures. This also fits with conceptual information known about the O-rings, such as lower temperatures causing them to shrink in size and provide a less complete seal against gases. Higher pressures could then also cause more erosion/blowby from a greater amount of gas in the same space in the system. While all of the distress events did occur at the highest pressure measurement of 200 psi, the general trend does not seem as strong as with temperature. For example, 200 psi pressure measurement was also the most common value of this variable generally. However, temperatures around the midpoint of the range ~65-75°F also had lower pressures, which may have helped them avoid any distress events.

The correlation between these variables can also be computed numerically for a more thorough understanding. Although pressure is binned into multiples of 50 psi, resembling a categorical or ordinal variable, the ratio of one pressure to another is still meaningful. For example, 200 psi is known to be four times as much pressure as 50 psi. A pressure of zero psi is also meaningful, meeting the definition for a ratio variable. Since theoretically a pressure can take on any value, even if only multiples of 50 are recorded, it will still be treated as a continuous variable. The count of O-rings which experienced distress meets the same definition of a ratio variable but is by nature discrete and not continuous. For example, 1.5 O-rings cannot experience distress. Therefore, Spearman's rank correlation coefficient will be used.

```{r, warning=FALSE}
cat("Table 1. Correlation of Numerical Variables\n")
# generate correlation table without certain columns
cor(d[ , !c("Flight","Number","distress")], method = "spearman")
```

**Table 1** shows the correlation between the three numerical variables which varied in the data set over the course of the launches, being `Temp`, `Pressure`, and `O.ring`. As suspected in **Figure 3**, pressure had a positive correlation with O-ring distress incidents. This supports the previous observation that higher pressures were associated with both launches with and without distress events, but lower pressures almost always were on flights without any distress events. 

Temperature had a stronger negative correlation with O-ring distress events. This continues to follow the trend shown in **Figures 2** and **3** where flights which experienced any distress were generally at colder temperatures. Finally, we can also see that temperature and pressure have a much smaller and slightly positive correlation with each other. This is in agreement with physical science, where gas enclosed in fixed volume will grow to higher pressures if the temperature is increased. Since the function of the O-ring is to provide a seal, i.e. a fixed volume, it would be reasonable that there is a slight positive correlation between the two.

## Modeling

```{r, include = FALSE}
# turn data into a data.frame from data.table
# for later analysis
d <- data.frame(d)
```

With the given context, there are different potential approaches to estimate the relationship between the environmental variables and O-ring distress. Given that each O-ring is discrete, a Poisson or Binomial distribution could be used depending on the scenario and modeling goals. The Poisson distribution describes processes which, among other things, occur over an interval at a constant average rate. This could be used to model the expected count of O-rings which fail for a series of Space Shuttle launches. However, the focus on the average over time is not particularly useful to estimate failure in a specific future launch. The Binomial distribution, on the other hand, describes processes which have a sequence of binary events, such as success or failure of multiple O-rings within a single Shuttle launch. This would be much more fitting for predicting a probability of failure on a single launch in the future, such as the *Challenger* at the start of its final mission. Because of this, only the Binomial distribution will be used in this investigation.

For both the Poisson and Binomial distributions, however, an assumption of independence is required. This would require that each Space Shuttle launch is independent and identically distributed (IID) compared to all others. In general this should be true, although there may be minor reasons why this is violated. For example, over time Shuttle technology or launch procedures may have been improved based on prior experience. This would be equivalent to a distributional change over time. In addition, part of the reason SRBs were recovered were not just for observing O-ring damage, but also for refurbishment and potential re-use in later launches. Having sets of launches with the same SRBs could result in sets of correlated observations that are different from other flights. It could also be expected that re-used SRBs could in general cause greater O-ring distress than those being newly used. This would be a notable violation since it can involve the observed outcome of O-ring damage along with its relationship to environmental conditions. With either a Poisson or Binomial distribution, it must be assumed to be negligent. In the future, knowledge of which SRBs were re-used for which launches could be incorporated into modeling for a more accurate estimate.

These modeling options also require that in addition to each Shuttle launch being IID, each of the six O-rings for any given flight must be IID as well. If the O-rings were not independent, the damage of a given O-ring would not only be a function of the environmental variables but also of the damage of other O-rings. Consequently, there would not be just one relationship between the O-ring damage and the environment, but multiple relationships depending on which O-ring was under consideration. 

Just like with the IID Shuttle launches, this is avoided with the assumption of independence however it may not be completely valid. For example, the six O-rings are spread out across two SRBs for each launch. It could be expected that there are small differences from between specific SRBs with respect to the stress the O-rings experience. For example, if an O-ring in one SRB had minor manufacturing differences that put more stress on its O-rings then these observations would be correlated. The three O-rings in the other SRB, however, may have their own correlation based on its own unique qualities. Therefore, in reality there is likely some relationship between the O-rings within a single SRB that is ignored for the sake of the model.

```{r}
# Determine fraction of O-rings that failed for each launch
d$O.ring.fraction <- d$O.ring / d$Number
```

With the assumption of independence, two different uses of the Binomial distribution will be investigated. The first will model *n* = 6 IID O-rings for flight, each with an identical probability *p* of experiencing distress. The set of O-rings within each launch and the series of all Shuttle launches will then make up a Bernoulli process. This will be referred to as the "full" outcome throughout the rest of this report. The second will investigate only the probability of a launch experiencing *any* O-ring distress by use of the binary `distress` variable created during the EDA. This will model the reduced *n* = 1 Bernoulli trial of whether at least one O-ring was distressed on a launch, with a single probability *p* of this occurring. This is a special case of the Binomial distribution known as the Bernoulli distribution, and will be referred to as the "binary" outcome throughout this report.

An advantage of the former is that it uses all available information in the dataset, potentially estimating a more accurate relationship between O-ring failure and environmental conditions. It also can provide a more useful prediction for the outcome of the *Challenger* disaster, since any design-flaw failure in a secondary O-ring when the primary is distressed would cause a catastrophe. Having an estimate of a specific number of primary O-rings which fail would then be proportional to the probability of the catastrophe. The use of the binary `distress` outcome, however, only estimates if at least one O-ring will fail without regard to how many. Still, because of its simpler nature it can potentially be more precise when the number of observations is so low. Therefore, the full model will be used to provide a primary estimate of the disaster's probability, with the binary model used as a spot-check against its results.

There are two explanatory variables of interest, temperature and pressure. To test whether either of these are significant, a series of three models will be created.

$$logit(\pi) = \beta_0$$
The first, shown above, will only include an intercept and serve as the null hypothesis where neither temperature nor pressure have a relationship with O-ring distress. This equivalent to $\beta_{T} = \beta_{P} = 0$. 

As common with Binomial distributions, a logit link function is used to relate the dependent and independent variables. The logit is defined as $logit(\pi) = ln(\frac{\pi}{1 - \pi})$ where $\pi$ is the probability of the outcome of interest. In the full model, this is the probability of failure of a single O-ring while in the binary model it is the probability of failure in at least one O-ring in a launch.

$$logit(\pi) = \beta_0 + \beta_{T}*T$$
The second, shown above, will include temperature as the sole independent variable denoted by $T$. This is motivated by Morton Thiokol's pre-launch investigation of O-ring condition deteriorating at lower temperatures using the same dataset. Later, it will be shown how their mistakes in statistical modeling lead to an underestimate of the probability of catastrophe. 

$$logit(\pi) = \beta_0 + \beta_{T}*T + \beta_{P}*P$$
Finally, the pressure $P$ will be included along with $T$ in the third model to see if both environmental conditions are also significantly related to O-ring distress. 

```{r}
# Null hypothesis, no relationship between env and o-rings
binary_null <- glm(formula = distress ~ 1, data = d, 
                   family = binomial(link = logit))

# First alternative hypothesis, only temp has relationship
binary_temp <- glm(formula = distress ~ Temp, data = d, 
                   family = binomial(link = logit))

# Second alternative hypothesis, both env variables have relationship
binary_both <- glm(formula = distress ~ Temp + Pressure, data = d,
                   family = binomial(link = logit))

# Null hypothesis, no relationship between env and o-rings
binom_null <- glm(formula = O.ring.fraction ~ 1, data = d, 
                  family = binomial(link = logit), weights = Number)

# First alternative hypothesis, only temp has relationship
binom_temp <- glm(formula = O.ring.fraction ~ Temp, data = d, 
                  family = binomial(link = logit), weights = Number)

# Second alternative hypothesis, both env variables have relationship
binom_both <- glm(formula = O.ring.fraction ~ Temp + Pressure, data = d,
                  family = binomial(link = logit), weights = Number)
```

The same set of three models will be used for both the full and binary outcome variables, shown above. Note that for the full models, instead of creating *n* = 6 unique data points for the O-rings of each launch, the fraction of O-ring failures in a single launch was used as the dependent variable. When paired with the weights of each observation, which is constant at six O-rings for all launches, this produces the same point estimate of model coefficients. 

A likelihood ratio test (LRT) will be used to determine whether a model associated with an alternative hypothesis is a significant improvement over the null hypothesis model. If the first alternative hypothesis is significant, i.e. involving only $T$, then the second alternative hypothesis including both $T$ and $P$ will be assessed against it. 

LRTs compare the likelihood of the alternative hypothesis model against the null hypothesis model. In general, an LRT is mathematically expressed as:

$$-2log(\Lambda) = -2log( \frac{L(\hat{\mathbf{\beta}}^{(0)} | y_1, \dots, y_n)}{L(\hat{\mathbf{\beta}}^{(a)} | y_1, \dots, y_n)} = -2\sum y_i log\left( \frac{\hat{\pi}_i^{(0)}}{\hat{\pi}_i^{(a)}} \right) + (1 - y_i ) log\left( \frac{1- \hat{\pi}_i^{(0)}}{1- \hat{\pi}_i^{(a)}} \right)$$
Where:

- $\Lambda$ is the likelihood ratio test statistic

- $L()$ is the likelihood function of the estimated model given the data, which in this case involves cross entropy between the predicted probabilities and binary true values. 

- $\hat{\pi}_i^{(0)}$ is the estimated probability of success of Bernoulli trial $i$ under the null hypothesis $H_0$

- $\hat{\pi}_i^{(a)}$ is the estimated probability of success of Bernoulli trial $i$ under the alternative hypothesis $H_a$

```{r}
alpha <- 0.05
significance_level <- alpha / 4
```

Because of these planned, repeated hypothesis tests, the Bonferroni correction will be applied to strengthen the level of significance required to reject the null hypothesis. Starting from $\alpha$ = 0.05, with four comparisons between the alternative and null hypothesis models, the new p-value required for significance will be $\alpha = 0.05 / 4 = 0.0125$.

```{r}
binary_temp_test <- anova(binary_null, binary_temp, test = "LRT")
binom_temp_test <- anova(binom_null, binom_temp, test = "LRT")
```

When performing the LRTs including only temperature in the alternative hypothesis, p-values of `r round(binary_temp_test[["Pr(>Chi)"]][2], 4)` and `r round(binom_temp_test[["Pr(>Chi)"]][2], 4)` are found for the binary and full models respectively. The binary model is definitively past the threshold required for significance of `r significance_level`, but the full model is very slightly above. While this would normally result in a failure to reject the null hypothesis, often times the choice of interpretation is dependent on the testing context and broader considerations. In particular, methods for defining significance should be viewed as guidelines rather than hard and fast rules. In this case, the Bonferroni correction is known for being overly conservative in that it guarantees the family wise Type I error rate will *always* be less than the desired level of $\alpha$. In particular, strict adherence to the Bonferroni correction is known for higher Type II error rates, or failure to reject a false null hypothesis. The significance of temperature in the binary model also suggests this could be the case if judging the borderline full model as non-significant. Therefore, for being nearly at the threshold and with these considerations, this result will still be taken as significant. 

```{r}
binary_pressure_test <- anova(binary_temp, binary_both, test = "LRT")
binom_pressure_test <- anova(binom_temp, binom_both, test = "LRT")
```

With the first pair of alternative hypotheses being failed to be rejected, they now will take the position of the null hypothesis for comparison against including pressure. For these LRTs, p-values of `r round(binary_pressure_test[["Pr(>Chi)"]][2], 4)` and `r round(binom_pressure_test[["Pr(>Chi)"]][2], 4)` are found for the binary and full models respectively. Unlike the previous borderline case, both of these are well above the Bonferroni corrected significance level as well as the original $\alpha$. Therefore, the null hypothesis that pressure does not have a significant relationship with O-ring condition is failed to be rejected. Moving forward, the temperature-only models will be used for further investigation and prediction.

Before use of the models for specific tasks, the fitted parameters within the model should also be examined. In this case, $\beta_T$ may be interpreted for the relationship between temperature and O-ring failure. When doing so with any link function other than the identity link, changes in an independent variable must account for additional transformation(s) to find the change in the dependent variable. 

$$OR = \frac{Odds_{T + c}}{Odds_T} = \frac{\pi_{T+c}}{1-\pi_{T+c}}*\frac{1-\pi_T}{\pi_T}= \frac{exp(\beta_0 + \beta_T*(T + c))}{exp(\beta_0 + \beta_T*T)} = exp(\beta_T * c)$$

With a logit link function, changes in an independent variable are most commonly interpreted through the odds ratio (OR), with a derivation for this context shown above. Specifically, the example shown describes the OR as a function of a *c* °F increase in temperature. Previously, the coefficient of temperature $\beta_T$ was estimated to be about `r round(binary_temp[["coefficients"]][["Temp"]], 3)` in the binary model and `r round(binom_temp[["coefficients"]][["Temp"]], 3)` in the full model. Both are negative, implying that generally as temperature increases then the estimated odds of O-ring failure decreases. Conversely, as temperature decreases then the estimated odds of O-ring failure increases. 

```{r}
c <- -10
binary_or <- exp(c*binary_temp[["coefficients"]][["Temp"]])
binom_or <- exp(c*binom_temp[["coefficients"]][["Temp"]])
```

For a 10°F decrease in temperature, the OR would be about `r round(binary_or, 1)` in the binary model and `r round(binom_or, 1)` in the full model. Put into context, this implies that with a 10°F lower temperature the odds of *any* O-ring failure increases by `r round(binary_or, 1)` and the odds of a specific O-ring failure increases by `r round(binom_or, 1)`. This change is irrespective of the starting temperature, and as the change in temperature grows more negative then the odds of O-ring failure will continue to increase.

For future plotting, visualization of confidence intervals will sometimes be included. While the previous hypothesis tests were LRTs, when this is done the Wald confidence intervals will be used instead.

```{r, warning = FALSE}
# create sequence of temperatures to show on plot
temps <- data.frame(Temp = seq(30,82,1))

# make predictions on the sequence of temperatures
binary_prob <- predict(object = binary_temp, newdata = temps, 
                       type='response', se=TRUE)
binom_prob <- predict(object = binom_temp, newdata = temps, 
                       type='response', se=TRUE)

# pi values
binary_pis <- binary_prob$fit
binom_pis <- binom_prob$fit

# 2.5th percentile
binary_lower_ci <- binary_pis - qnorm(p = 1 - alpha/2)*binary_prob$se.fit
binom_lower_ci <- binom_pis - qnorm(p = 1 - alpha/2)*binom_prob$se.fit
# 97.5th percentile
binary_upper_ci <- binary_pis + qnorm(p = 1 - alpha/2)*binary_prob$se
binom_upper_ci <- binom_pis + qnorm(p = 1 - alpha/2)*binom_prob$se

binary_df <- data.frame(temp = temps$Temp, pi = binary_pis, 
                        lwr = binary_lower_ci, upr = binary_upper_ci)
binom_df <- data.frame(temp = temps$Temp, pi = binom_pis, 
                        lwr = binom_lower_ci, upr = binom_upper_ci)

binary_color <- "darkgreen"
binom_color <- "blue"

ggplot() + 
  geom_line(data = binary_df, aes(x = temp, y = pi, color = binary_color),
            show.legend = TRUE) + 
  geom_line(data = binary_df, aes(x = temp, y = lwr), 
            color = binary_color, linetype = "dotted") + 
  geom_line(data = binary_df, aes(x = temp, y = upr),
            color = binary_color, linetype = "dotted") +
  geom_line(data = binom_df, aes(x = temp, y = pi, color = binom_color),
            show.legend = TRUE) + 
  geom_line(data = binom_df, aes(x = temp, y = lwr), 
            color = binom_color, linetype = "dotted") + 
  geom_line(data = binom_df, aes(x = temp, y = upr),
            color = binom_color, linetype = "dotted") +
  ylim(0,1) + 
  ggtitle("Figure 4. Probability of O-Ring Failure by Temperature") + 
  ylab(expression(pi)) + 
  xlab("Temperature (°F)") + 
  geom_hline(yintercept = 1.0, linetype = "dotted") +
  geom_hline(yintercept = 0.0, linetype = "dotted") +
  geom_vline(xintercept = 36, linetype = "solid") +
  geom_point(data = d, aes(x = Temp, y = ifelse(distress==TRUE, 1, 0)), 
             shape = 17) +
  scale_color_identity(name = "Model",
                       labels = c("Binary", "Full"),
                       breaks = c(binary_color, binom_color),
                       guide = "legend")

```

**Figure 4** shows the probability of O-ring failure from both models over a range of temperatures. The Wald confidence intervals with $\alpha$ = 0.05 are shown as dotted lines for each model. Previous Shuttle launches are also included depending whether they did or did not have at least one O-ring failure, which is the dependent variable in the binary model. Finally, the vertical line represents the launch temperature of the *Challenger* disaster of 36°F.

As described previously the binary case represents the probability that *any* of the six O-rings will fail during launch, while the full case represents the probability of a single O-ring within the six failing. Shown earlier with the OR, a decrease in temperature generally increases the probability of O-ring failure in both models. This is visualized here as the probabilities from both models increase at changing rates as temperature decreases. The binary model has a larger increase in probability at moderate temperatures than the full model, which is intuitive as it only tracks whether at least one out of six O-rings will fail rather than the probability of a single specific O-ring. 

At the launch temperature of the *Challenger*, the binary model predicts there is a near guarantee that at least one O-ring will fail. The full model likewise predicts a high probability of each individual O-ring failing at nearly 75%. With the O-rings being assumed IID as required for the Binomial distribution, the expected number of O-ring failures on a single launch is then given by:

$$E[Y] = n * p = 6 * \hat{\pi}(T)$$
where *n* = 6 from the six IID O-rings on each launch and *p* = $\hat{\pi}(T)$ is the estimated probability of an individual O-ring failing at a specific temperature as given by the full model. Again, by design only the full model can estimate how many O-rings on a given launch may fail as the binary model is only useful for understanding if *at least* one O-ring will fail, without regard to the total number.

The key feature of Morton Thiokol's original analysis was that all launches which had zero O-ring failures were omitted.$^{[1]}$ This was justified by assuming they had nothing to offer to an understanding why O-rings may fail, or if temperature could be related. However, to falsify the claim that temperature is related to O-ring failure, both successes and failures of O-rings must be considered. Attempting to do so only based on examples of O-rings which failed, or conversely O-rings which did not fail, is impossible. Especially when a causal effect is suspected, an analysis which attempts to do so does not meet fundamental criteria of science. 

```{r}
d_morton <- d[d$O.ring > 0, ]
binom_temp_morton <- glm(formula = O.ring.fraction ~ Temp, data = d_morton, 
                         family = binomial(link = logit), weights = Number)
binom_prob_morton <- predict(object = binom_temp_morton, newdata = temps,
                             type='response', se=TRUE)
binom_pis_morton <- binom_prob_morton$fit
binom_lower_ci_morton <- binom_pis_morton - 
  qnorm(p = 1 - alpha/2)*binom_prob_morton$se.fit
binom_upper_ci_morton <- binom_pis_morton + 
  qnorm(p = 1 - alpha/2)*binom_prob_morton$se.fit
binom_df_morton <- data.frame(temp = temps$Temp, pi = binom_pis_morton, 
                              lwr = binom_lower_ci_morton, 
                              upr = binom_upper_ci_morton)
binom_color_morton <- "red"
```

An identical subset of the launch data as used by Morton Thiokol is created above. With this, the same structure of the full model can be fit using to determine how it may impact analysis. 

```{r, warning = FALSE}
mode <- function(v) {
  #' Function to find the statistical mode of a vector and return the result
  #' Inputs:
  #'  vector v: vector to find the mode of
  #' Outputs:
  #'  statistical mode of the vector v
   uniqv <- unique(v)
   return(uniqv[which.max(tabulate(match(v, uniqv)))])
}

num_O_rings <- mode(d$Number)

# estimated number of failed O-rings by temperature
binom_df$num_failed_est <- num_O_rings*binom_df$pi
binom_df$num_failed_lwr <- num_O_rings*binom_df$lwr
binom_df$num_failed_upr <- num_O_rings*binom_df$upr
binom_df_morton$num_failed_est <- num_O_rings*binom_df_morton$pi
binom_df_morton$num_failed_lwr <- num_O_rings*binom_df_morton$lwr
binom_df_morton$num_failed_upr <- num_O_rings*binom_df_morton$upr

ggplot() + 
  geom_line(data = binom_df, 
            mapping = aes(x = temp, y = num_failed_est, color = binom_color)) + 
  geom_line(data = binom_df, aes(x = temp, y = num_failed_lwr), 
            color = binom_color, linetype = "dotted") + 
  geom_line(data = binom_df, aes(x = temp, y = num_failed_upr),
            color = binom_color, linetype = "dotted") +
  geom_line(data = binom_df_morton, 
            mapping = aes(x = temp, y = num_failed_est, 
                          color = binom_color_morton)) + 
  geom_line(data = binom_df_morton, aes(x = temp, y = num_failed_lwr), 
            color = binom_color_morton, linetype = "dotted") + 
  geom_line(data = binom_df_morton, aes(x = temp, y = num_failed_upr),
            color = binom_color_morton, linetype = "dotted") +
  ggtitle("Figure 5. Expected Number of O-Ring Incidents by Temperature") + 
  ylab("Number of O-Ring Failures per Launch") + xlab("Temperature (°F)") + 
  geom_point(data = d, mapping = aes(x = Temp, y = O.ring), shape = 17) +
  geom_vline(xintercept = 36, linetype = "solid") +
  geom_hline(yintercept = num_O_rings, linetype = "dotted") +
  geom_hline(yintercept = 0, linetype = "dotted") + ylim(0, num_O_rings) + 
  scale_color_identity(name = "Full Model",
                       labels = c("Subset Data", "Full Data"),
                       breaks = c(binom_color_morton, binom_color),
                       guide = "legend")
```

**Figure 5** shows the number of predicted O-ring failures per launch over a range of temperatures, along with the Wald confidence intervals. The two curves are both generated from the full model, one which uses only the data subset Morton Thiokol considered and one on the full data. Like **Figure 4**, the vertical line indicates the launch temperature on the morning of the *Challenger* disaster of 36 °F. 

Most notably, using only the subset data implies that temperature is not related to O-ring failure. Instead, the predicted number of failures is constant around one. At higher and lower temperatures the Wald confidence interval grows wider, but this is due to a lack of observations outside of the six which remain. This also highlights that in addition to being biased, this subset is also incredibly small. When already starting with rare events such as Space Shuttle launches, any further reduction in dataset size must be well motivated. As a result, with only six observations and two parameters in the model, the available degrees of freedom are nearly exhausted.

On the other hand, the full model with all data tracks previous Shuttle launches well. The only point which does not fit this curve is the observation with two O-ring failures at a temperature of about ~75°F. Even though this point is far from the predictions of the model, it does not appear to have skewed its fit. 

Importantly, the model with all data predicts that at about four out of the six primary O-rings would fail at the launch temperature of the *Challenger*. This is an key result, as no previous launch had greater than two O-ring failures observed. As mentioned earlier, these six primary O-rings were intended to be redundant with six secondary O-rings. However, early in the Shuttle program it was known that a phenomena specific to the secondary O-rings could occur which rendered them useless at the start of each launch. If this happened at the same time as a primary O-ring failed, then it was already predicted that the result would be a catastrophe. Therefore, with at least four primary O-rings expected to fail at launch, the *Challenger* disaster would only have been averted if all four of the secondary O-rings to these did not also start the launch non-functional. 

Still, this estimate must be qualified by the large confidence interval ranging from one to all six primary O-rings failing. In both **Figures 4** and **5**, the width of the Wald confidence intervals generally increase as temperature decreases in the full model. In the binary model of **Figure 4**, however, the confidence interval grows and then shrinks as temperatures decreases. In fact, at the launch temperature of 36°F the width is extremely small in the binary model, representing high confidence that at least one O-ring will fail. However, this is likely an example of a "zero-width" interval being estimated due to failing to meet the Wald interval assumptions. The continued growing of the confidence interval of the full model occurs due to a lack of observations at lower temperatures, coupled with the predicted probabilities being in the middle of the 0 and 100% bounds.

```{r}
binwidth <- 0.1

ggplot() +
  geom_histogram(data = binary_df, mapping = aes(x = pi, fill = binary_color),
    binwidth = binwidth, center = binwidth/2, alpha=0.5, show.legend = TRUE) + 
  geom_histogram(data = binom_df, mapping = aes(x = pi, fill = binom_color),
    binwidth = binwidth, center = binwidth/2, alpha=0.5, show.legend = TRUE) + 
  ggtitle("Figure 5. Distribution of Predicted Probabilities") +
  xlab(expression(hat(pi))) + ylab("Count") +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) + ylim(0,30) +
  scale_fill_identity(name = "Model",
                      labels = c("Binary", "Full"),
                      breaks = c(binary_color, binom_color),
                      guide = "legend")
```

The validity of the Wald intervals can be examined via **Figure 5**. Specifically, the Wald interval assumes the outcome variable follows a normal distribution and has a large sample size. Earlier in the exercise, however, a Binomial distribution was assumed for a more appropriate and useful model. While Binomial distributions resemble normal distributions when *n* is large and *p* is near 0.5, these circumstances are not usually met and lead to "chaotic coverage" of the Wald interval.$^{[3]}$ Also as noted earlier, Space Shuttle launches are rare events and the overall size of the dataset is small at less than thirty observations. 

The effect of these differences can be seen in **Figure 5** where $\hat{\pi}$ has fewer observations towards the center of its range, with a greater number on its edges. This is true for both the binary and full models. However, this is the opposite of a normal distribution where the median value is the most often observed. Combined with the previous differences, the Wald assumptions are not well met and it is likely they are not an accurate estimate. With all of these differences, the Wald intervals should not be considered precise estimates of the standard error.

As done with the previous hypothesis tests, the profile LR confidence interval can be constructed instead of the Wald. Unlike the Wald, the LR confidence interval assumes a $\chi^2$ distribution in the outcome. This is much more easily approximated by the Binomial distribution the data was assumed to follow for modeling.

```{r}
beta_0 <- fail_model$coefficients[1]
beta_1 <- fail_model$coefficients[2]

# Create a K-matrix for temp = 31 degrees

K <- matrix(data = c(1,31) , nrow = 1, ncol = 2)

# Calc -2log(lambda)
lc <- mcprofile(object=fail_model, CM=K)
ci_logit_profile <- confint(object=lc, level=0.95)

interval <- exp(ci_logit_profile$confint)/(1 + exp(ci_logit_profile$confint))

wald_int_profile <- wald(object=lc)

wald_ci_exp <- confint(wald_int_profile , level = .95)

wald_lower_exp <- wald_ci_exp$confint$lower
wald_upper_exp <- wald_ci_exp$confint$upper

wald_interval_lower <- exp(wald_lower_exp)/(1 + exp(wald_lower_exp))
wald_interval_upper <- exp(wald_upper_exp)/(1 + exp(wald_upper_exp))
```

For the LRT test and at 31°F, the odds of any `O.ring` failure has a 95% confidence interval of `r interval$lower` and `r interval$upper`. The Wald test under the same conditions has the 95% confidence interval of any `O.ring` failure between `r wald_interval_lower` and `r wald_interval_upper`. This demonstrates that the LRT is more conservative than the Wald and should be preferred.

(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31° and 72°.27

First, the calculations will be set up.

```{r}
bs_data_1 <- data.frame(c(31))
colnames(bs_data_1) = 'Temp'

bs_data_2 <- data.frame(c(72.27))
colnames(bs_data_2) = 'Temp'

vector_pi <- NA
vector_lower_ci <- NA
vector_upper_ci <- NA

vector_pi2 <- NA
vector_lower_ci2 <- NA
vector_upper_ci2 <- NA

suppressWarnings(for(i in 1:1000) {
  
  # generating data from repeated sampling, only temperature will be used
  bootstrap_temps <- sample_n(d, size=23, replace=TRUE)$Temp
  
  # make predictions on data with original model
  bootstrap_outcomes <- predict(object = fail_model, data = bootstrap_temps, 
                                type = "response", family = binomial(link = "logit"))
  
  # put bootstrap data together for fitting new model
  bootstrap_d <- data.frame("Temp" = bootstrap_temps, 
                            "distress" = bootstrap_outcomes)
  
  # fit new model using bootstrapped data
  model <- glm(formula = distress ~ Temp, 
               data = bootstrap_d, family = binomial(link = logit))
  
  # make predictions using bootstrap model at temperatures of interest
  prediction_1 <- predict(object=model, 
                          newdata=bs_data_1, type='response', se=TRUE)
  vector_pi[i] <- prediction_1$fit
  #note that we are supposed to do a 90% CI
  vector_upper_ci[i] <- prediction_1$fit + 1.645*prediction_1$se.fit
  vector_lower_ci[i] <- prediction_1$fit - 1.645*prediction_1$se.fit
  #doing the same for 72 degrees
  prediction_2 <- predict(object=model, 
                          newdata=bs_data_2, type='response', se=TRUE)
  vector_pi2[i] <- prediction_2$fit
  vector_upper_ci2[i] <- prediction_2$fit + 1.645*prediction_2$se.fit
  vector_lower_ci2[i] <- prediction_2$fit - 1.645*prediction_2$se.fit
})

temp_30_pis <- data.frame(data.frame(vector_pi), 
                          data.frame(vector_lower_ci), 
                          data.frame(vector_upper_ci))
colnames(temp_30_pis) = c('pi','lwr', 'upr')

temp_72_pis <-data.frame(data.frame(vector_pi2), 
                         data.frame(vector_lower_ci2), 
                         data.frame(vector_upper_ci2))
colnames(temp_72_pis) = c('pi','lwr', 'upr')

#calculating the standard deviations

pi_hat30_std_dev <- sd(temp_30_pis$pi)
pi_hat72_std_dev <- sd(temp_72_pis$pi)

#estimated mean
mu_hat_30 <- mean(temp_30_pis$pi)
mu_hat_72 <- mean(temp_72_pis$pi)

alpha <- .1
ci_30 <- mu_hat_30 + qnorm(p = c(alpha/2,1-alpha/2))*pi_hat30_std_dev
ci_72 <- mu_hat_72 + qnorm(p = c(alpha/2,1-alpha/2))*pi_hat72_std_dev
```

Next, the figures will be created.

```{r}
# 30 degrees 
plt_30 <- ggplot(data = temp_30_pis) +
aes(x = pi) +
geom_histogram(binwidth=0.01, fill="blue", color="blue", alpha=0.3) +
geom_vline(xintercept = ci_30 , color = 'darkorange') + 
labs(title = "Figure 7. Histogram of Bootstrapped Probabilities of Failure at 31°F", 
     caption = ("Vertical lines denote the 90% CI")) +
  ylab("Count, Logarithm") +
  xlab(expression(pi)) +
  scale_y_log10()

#72 degrees
plt_72 <- ggplot(data = temp_72_pis) +
aes(x = pi) +
geom_histogram(binwidth=0.01, fill="purple", color="purple", alpha=0.3) +
geom_vline(xintercept = ci_72 , color = 'darkorange') + 
labs(title = 
       "Figure 8. Histogram of Bootstrapped Probabilities of Failure at 72.27°F", 
     caption = ("Vertical lines denote the 90% CI")) +
    ylab("Count") +
  xlab(expression(pi))

plt_30
plt_72
```

We can visualize a histogram of the results of the bootstrap (100,000 trials each) for 31°F in **Figure 7** and for 72°F in **Figure 8**. Using data in these figures, we can calculate that:

- The probability of an `O.ring` failure at 30°F are between `r ci_30[1] ` and `r ci_30[2] ` (with a 90% C.I.)

- The probability of an `O.ring` failure at 72.27°F are between `r ci_72[1] ` and `r ci_72[2] ` (with a 90% C.I.)

While these probabilities may be calculated to sometimes be less than one or greater than zero, it is known their true bounds are between [0, 1].

(f) Determine if a quadratic term is needed in the model for the temperature.

We'll create another glm model with a quadratic term and then compare the two models with an anova test as below:

```{r}
fail_model_quad <- glm(formula = distress ~ Temp + I(Temp^2), data = d,
                       family = binomial(link = logit))
anova_mod <- anova(fail_model, fail_model_quad , test = "Chisq")

p_val<- anova_mod$`Pr(>Chi)`[2]
```

The p-value from the anova test is `r p_val`, which is fairly large and greater than 0.05. Thus, we conclude that there is not enough evidence that the quadratic model is an improvement over the the linear model. 

**Part 4 (10 points)**

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results, conduct model diagnostics and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

```{r}
linear_model <- glm(distress ~ Temp, data = d, family = gaussian())
```

Shown above, the linear model also finds that temperature is significant for the presence of a O-ring distress event. Like the previous models there is an inverse relationship between the two variables, where the likelihood of a distress event increases as the temperature decreases. Specifically, with all else constant, increasing the temperature by 1°F would decrease the probability of a distress event by ~3.74%.

Even though a similar result is found, this model should not be used instead of the binary logistic regression model. This is because the linear model does not have bounds on the output, so that sometimes a probability greater than one or less than zero may be predicted. In this case, a probability of one is reached at about `r round((1-coef(linear_model)["(Intercept)"])/coef(linear_model)["Temp"], 2)`°F while a probability of zero is reached at about `r round((0-coef(linear_model)["(Intercept)"])/coef(linear_model)["Temp"], 2)`°F. Given that there were recorded launches both above and below these temperatures, this is an undesirable property of the model.

In addition, the linear model assumes homoscedasticity, or that the variance in the model's residuals is constant both over the range of the output as well as the domain of its inputs. 

```{r}
linear_model %>%
  ggplot(aes(x = linear_model$fitted.values, 
             y = sqrt(abs(linear_model$residuals/sd(linear_model$residuals))))) + 
  geom_point() + 
  stat_smooth(color="red", se=FALSE) +
  labs(
    title = "Figure 9. Predicted Probabilities vs. Standardized Residuals",
    x = "Predicted Values",
    y = "sqrt(|Standardized Residuals|)")
```

Shown in *Figure 9*, the standardized residuals are not constant over the range of the output variable in the model. This does not meet the homoscedasticity assumption, because the output variable is itself a function of the input variables. This result was expected, however, as it can be derived that the variance in a random variable from a binomial distribution is a function of that variable itself, or $Var(\pi) \propto \pi*(1-\pi)$. It can be further seen that the maximum variance is observed around $\pi \approx 0.5$, which follows the prediction from the variance equation. Given that the homoscedasticity assumption is not met, there is further evidence of preferring the logistic model to the linear regression.

A final drawback of the linear model is that, as shown in its name, it only considers linear combinations of the input variables to predict the output variables. However, the logistic regression model also follows this linear combination form, and so is not any more or less desirable for this reason alone. However, given that in this scenario the previous two drawbacks were so large, the logistic regression model should be preferred instead. 

**Part 5 (10 points)**

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.

The final model is the logistic regression which uses temperature as the single explanatory variable. The model is of the form

$$\mathrm{logit(\hat{\pi})=\hat{\beta_0} + \hat{\beta_1}(Temp)}$$
```{r}
beta_0_hat <- fail_model$coefficients[1]
beta_1_hat <- fail_model$coefficients[2]
```

The intercept in the model, $\beta_0$, had the value `r beta_0_hat` while the coefficient of `Temp` had the value `r beta_1_hat`. The coefficient on the `Temp` variable can be interpreted as decreasing the odds of any O-ring failures by `r exp(beta_1_hat)` with every 1°F increase in temperature and all else constant.

This is a key finding. It underscores the importance git status
of atmospheric temperature on the day of the launch, and that lower temperatures would increase the odds of an O-ring failure occurring. This is supported by Dalal et al. when they state the resiliency of an O-ring is highly dependent on temperature. 

On the day of the launch the temperature was 31°F. At this point, the predicted the probability of any O-ring failure is `r exp(beta_0_hat + beta_1_hat*31 )/(1 + exp(beta_0_hat + beta_1_hat*31 ))`, with a 95 % confidence interval (LRT) as `r interval$lower` and `r interval$upper`.

Therefore, with the 95% confidence interval so high, at least one O-ring is very likely to fail under these conditions. If this was known, the launch could have been postponed for warmer weather when there was a lower probability of a failure. We can conclude that the catastrophe could have been avoided had NASA managers appropriately analyzed the data by factoring in both the successful and failed launches. 

\newpage

Citation:

[1] Dalal, Siddhartha R., et al. “Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure.” Journal of the American Statistical Association, vol. 84, no. 408, [American Statistical Association, Taylor & Francis, Ltd.], 1989, pp. 945–57, https://doi.org/10.2307/2290069.

[2] Presidential Commission on the Space Shuttle *Challenger* Accident (1986), *Report of the Presidential Commission on the Space Shuttle Challenger Accident* (Vols. 1 & 2), Washington, D.C.: Author. 

[3] Lawrence D. Brown, T. Tony Cai, Anirban DasGupta "Interval Estimation for a Binomial Proportion," Statistical Science, Statist. Sci. 16(2), 101-133, (May 2001)





